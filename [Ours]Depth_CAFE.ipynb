{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edb86687",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem .",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem ."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "xFormers not available\n",
      "xFormers not available\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 8.5534, Train Acc: 0.4846\n",
      "Val Loss: 0.8900, Val Acc: 0.8242\n",
      "Epoch: 2\n",
      "Train Loss: 8.1503, Train Acc: 0.5237\n",
      "Val Loss: 0.8119, Val Acc: 0.8533\n",
      "Epoch: 3\n",
      "Train Loss: 7.8923, Train Acc: 0.5829\n",
      "Val Loss: 0.7879, Val Acc: 0.8571\n",
      "Epoch: 4\n",
      "Train Loss: 7.9463, Train Acc: 0.5460\n",
      "Val Loss: 0.7403, Val Acc: 0.8793\n",
      "Epoch: 5\n",
      "Train Loss: 7.8081, Train Acc: 0.5852\n",
      "Val Loss: 0.7429, Val Acc: 0.8740\n",
      "Epoch: 6\n",
      "Train Loss: 7.7623, Train Acc: 0.5918\n",
      "Val Loss: 0.7249, Val Acc: 0.8847\n",
      "Epoch: 7\n",
      "Train Loss: 7.7938, Train Acc: 0.5689\n",
      "Val Loss: 0.7095, Val Acc: 0.8925\n",
      "Epoch: 8\n",
      "Train Loss: 7.7998, Train Acc: 0.5567\n",
      "Val Loss: 0.7132, Val Acc: 0.8878\n",
      "Epoch: 9\n",
      "Train Loss: 7.6463, Train Acc: 0.6106\n",
      "Val Loss: 0.7100, Val Acc: 0.8894\n",
      "Epoch: 10\n",
      "Train Loss: 7.6779, Train Acc: 0.5915\n",
      "Val Loss: 0.7096, Val Acc: 0.8894\n",
      "Epoch: 11\n",
      "Train Loss: 7.8156, Train Acc: 0.5658\n",
      "Val Loss: 0.7259, Val Acc: 0.8822\n",
      "Epoch: 12\n",
      "Train Loss: 7.6907, Train Acc: 0.6058\n",
      "Val Loss: 0.7236, Val Acc: 0.8828\n",
      "Epoch: 13\n",
      "Train Loss: 7.6834, Train Acc: 0.5887\n",
      "Val Loss: 0.7174, Val Acc: 0.8872\n",
      "Epoch: 14\n",
      "Train Loss: 7.6629, Train Acc: 0.5944\n",
      "Val Loss: 0.7019, Val Acc: 0.8941\n",
      "Epoch: 15\n",
      "Train Loss: 7.6927, Train Acc: 0.5794\n",
      "Val Loss: 0.7161, Val Acc: 0.8909\n",
      "Epoch: 16\n",
      "Train Loss: 7.6316, Train Acc: 0.5810\n",
      "Val Loss: 0.7104, Val Acc: 0.8988\n",
      "Epoch: 17\n",
      "Train Loss: 7.6192, Train Acc: 0.5785\n",
      "Val Loss: 0.7078, Val Acc: 0.8960\n",
      "Epoch: 18\n",
      "Train Loss: 7.5356, Train Acc: 0.6147\n",
      "Val Loss: 0.7084, Val Acc: 0.8944\n",
      "Epoch: 19\n",
      "Train Loss: 7.5186, Train Acc: 0.6163\n",
      "Val Loss: 0.7086, Val Acc: 0.8947\n",
      "Epoch: 20\n",
      "Train Loss: 7.5799, Train Acc: 0.5756\n",
      "Val Loss: 0.7074, Val Acc: 0.8978\n",
      "\n",
      "[Final Evaluation on FERPlus Test Set]\n",
      "Test Loss: 0.7824, Test Accuracy: 0.8881\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import clip\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.nn.modules.utils import _pair\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "from depth_anything_v2.dpt import DepthAnythingV2\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    category=UserWarning,\n",
    "    message=\".*antialias parameter.*\"\n",
    ")\n",
    "\n",
    "import timm\n",
    "\n",
    "CONVNEXT_NAME = \"convnext_large\"  # 백본\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Depth Model 로드 (경로는 사용자 환경에 맞게 수정)\n",
    "depth_model = DepthAnythingV2(encoder='vitl', features=256, out_channels=[256, 512, 1024, 1024])\n",
    "depth_model.load_state_dict(torch.load('/home/work/dhkim/fer/Depth-Anything-V2/checkpoints/depth_anything_v2_vitl.pth', map_location=device))\n",
    "depth_model = depth_model.to(device)\n",
    "depth_model.eval()\n",
    "\n",
    "class my_MaxPool2d(Module):\n",
    "    def __init__(self, kernel_size, stride=None, padding=0, dilation=1,\n",
    "                 return_indices=False, ceil_mode=False):\n",
    "        super(my_MaxPool2d, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride or kernel_size\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.return_indices = return_indices\n",
    "        self.ceil_mode = ceil_mode\n",
    "    def forward(self, input):\n",
    "        input = input.transpose(3, 1)\n",
    "        input = F.max_pool2d(input, self.kernel_size, self.stride,\n",
    "                             self.padding, self.dilation, self.ceil_mode,\n",
    "                             self.return_indices)\n",
    "        input = input.transpose(3, 1).contiguous()\n",
    "        return input\n",
    "\n",
    "class my_AvgPool2d(Module):\n",
    "    def __init__(self, kernel_size, stride=None, padding=0, ceil_mode=False,\n",
    "                 count_include_pad=True):\n",
    "        super(my_AvgPool2d, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride or kernel_size\n",
    "        self.padding = padding\n",
    "        self.ceil_mode = ceil_mode\n",
    "        self.count_include_pad = count_include_pad\n",
    "    def forward(self, input):\n",
    "        input = input.transpose(3, 1)\n",
    "        input = F.avg_pool2d(input, self.kernel_size, self.stride,\n",
    "                             self.padding, self.ceil_mode, self.count_include_pad)\n",
    "        input = input.transpose(3, 1).contiguous()\n",
    "        return input\n",
    "\n",
    "def generate_or_load_depth_map(image_path):\n",
    "    depth_map_path = image_path.replace('.png', '_depth_map.png').replace('.jpg', '_depth_map.png')\n",
    "    if os.path.exists(depth_map_path):\n",
    "        depth_map = cv2.imread(depth_map_path, cv2.IMREAD_GRAYSCALE)\n",
    "    else:\n",
    "        raw_img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if raw_img is None:\n",
    "            raise FileNotFoundError(f\"Input image not found at the specified path: {image_path}\")\n",
    "        \n",
    "        raw_img_rgb = cv2.cvtColor(raw_img, cv2.COLOR_GRAY2RGB)\n",
    "        with torch.no_grad():\n",
    "            depth = depth_model.infer_image(raw_img_rgb)\n",
    "        \n",
    "        normalized_depth = (depth - depth.min()) / (depth.max() - depth.min()) * 255.0\n",
    "        depth_map = normalized_depth.astype(np.uint8)\n",
    "        \n",
    "        cv2.imwrite(depth_map_path, depth_map)\n",
    "    return depth_map\n",
    "\n",
    "class RafDataset(data.Dataset):\n",
    "    def __init__(self, root_dir, phase, transform=None, apply_constraints=False):\n",
    "        self.root_dir = root_dir\n",
    "        self.phase = phase\n",
    "        self.transform = transform\n",
    "\n",
    "        label_path = os.path.join(root_dir, phase, 'label.csv')\n",
    "        df = pd.read_csv(label_path, header=None)\n",
    "\n",
    "        df.columns = ['filename', 'bbox'] + [f'c{i}' for i in range(10)]\n",
    "        counts = df[[f'c{i}' for i in range(10)]].astype(int)\n",
    "\n",
    "        if apply_constraints:\n",
    "            counts = counts.applymap(lambda x: 0 if x == 1 else x)\n",
    "            max_counts = counts.max(axis=1)\n",
    "            total_votes = counts.sum(axis=1)\n",
    "            valid_mask = max_counts > (total_votes / 2)\n",
    "            df = df[valid_mask].reset_index(drop=True)\n",
    "            counts = counts[valid_mask].reset_index(drop=True)\n",
    "\n",
    "        df['label'] = counts.values.argmax(axis=1)\n",
    "        valid_labels_mask = (df['label'] >= 0) & (df['label'] <= 7)\n",
    "        df = df[valid_labels_mask].reset_index(drop=True)\n",
    "        self.file_paths = df['filename'].values\n",
    "        self.labels = df['label'].values.astype(np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        img_filename = self.file_paths[idx]\n",
    "        img_path = os.path.join(self.root_dir, self.phase, img_filename)\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        image = image[:, :, ::-1]\n",
    "\n",
    "        depth_map = generate_or_load_depth_map(img_path)\n",
    "        depth_map = np.expand_dims(depth_map, axis=2)\n",
    "        image_with_depth = np.concatenate((image, depth_map), axis=2)  # (H,W,4)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image_with_depth = self.transform(image_with_depth)\n",
    "\n",
    "        return image_with_depth, label, idx, image_with_depth\n",
    "\n",
    "class RAFDBTestDataset(data.Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): RAF-DB 테스트 데이터셋의 루트 디렉토리. \n",
    "                            예: '/path/to/RAF-DB/DATASET/test'\n",
    "            transform (callable, optional): 데이터에 적용할 변환 함수.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.label_mapping = {\n",
    "            '1': 2,  # surprise -> FERPlus label 2\n",
    "            '2': 6,  # fear -> FERPlus label 6\n",
    "            '3': 5,  # disgust -> FERPlus label 5\n",
    "            '4': 1,  # happiness -> FERPlus label 1\n",
    "            '5': 4,  # sadness -> FERPlus label 4\n",
    "            '6': 3,  # anger -> FERPlus label 3\n",
    "            '7': 0   # neutral -> FERPlus label 0\n",
    "        }\n",
    "        self.file_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        # 각 레이블 디렉토리를 순회하며 파일 경로와 레이블을 수집\n",
    "        for label_str, mapped_label in self.label_mapping.items():\n",
    "            label_dir = os.path.join(root_dir, label_str)\n",
    "            if not os.path.isdir(label_dir):\n",
    "                continue\n",
    "            for fname in os.listdir(label_dir):\n",
    "                # depth map 파일(_depth_map.png)은 스킵\n",
    "                if '_depth_map' in fname:\n",
    "                    continue\n",
    "\n",
    "                # 이미지 확장자만 필터링\n",
    "                if fname.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    self.file_paths.append(os.path.join(label_dir, fname))\n",
    "                    self.labels.append(mapped_label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            image_with_depth (Tensor): 변환된 이미지 텐서 (4채널: RGB + Depth)\n",
    "            label (int): FERPlus 레이블 (0-6)\n",
    "            idx (int): 샘플 인덱스\n",
    "            image_with_depth (Tensor): 좌우 반전된 이미지 텐서 (여기서는 동일하게 반환)\n",
    "        \"\"\"\n",
    "        img_path = self.file_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"Image not found at path: {img_path}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # BGR에서 RGB로 변환\n",
    "\n",
    "        # Depth Map 로드 또는 생성\n",
    "        depth_map = generate_or_load_depth_map(img_path)  # H x W\n",
    "        depth_map = np.expand_dims(depth_map, axis=2)  # H x W x 1\n",
    "\n",
    "        # 이미지와 Depth Map 결합\n",
    "        image_with_depth = np.concatenate((image, depth_map), axis=2)  # H x W x 4\n",
    "\n",
    "        if self.transform:\n",
    "            image_with_depth = self.transform(image_with_depth)\n",
    "\n",
    "        # 좌우 반전은 테스트 시 필요 없으므로 동일하게 반환\n",
    "        return image_with_depth, label, idx, image_with_depth\n",
    "\n",
    "\n",
    "def Mask(nb_batch, num_classes=8, total_channels=3072):\n",
    "    channels_per_class = total_channels // num_classes\n",
    "    remainder = total_channels % num_classes\n",
    "\n",
    "    class_channel_counts = [channels_per_class] * num_classes\n",
    "    for i in range(remainder):\n",
    "        class_channel_counts[i] += 1\n",
    "\n",
    "    bar = []\n",
    "    for _ in range(nb_batch):\n",
    "        batch_bar = [0] * total_channels\n",
    "        active_channels = []\n",
    "        for count in class_channel_counts:\n",
    "            indices = random.sample(range(total_channels), count)\n",
    "            active_channels.extend(indices)\n",
    "        for idx in active_channels:\n",
    "            batch_bar[idx] = 1\n",
    "        bar.append(batch_bar)\n",
    "\n",
    "    bar = np.array(bar).astype(\"float32\")\n",
    "    bar = bar.reshape(nb_batch, total_channels, 1, 1)\n",
    "    bar = torch.from_numpy(bar).to(device)\n",
    "    return bar\n",
    "\n",
    "def supervisor(x, targets, cnum):\n",
    "    branch = x.reshape(x.size(0), x.size(1), 1, 1)\n",
    "    branch = my_MaxPool2d(kernel_size=(1, cnum), stride=(1, cnum))(branch)\n",
    "    branch = branch.reshape(branch.size(0), branch.size(1), -1)\n",
    "    loss_2 = 1.0 - torch.mean(torch.sum(branch, 2)) / cnum\n",
    "\n",
    "    mask = Mask(x.size(0), num_classes=8, total_channels=x.size(1))\n",
    "    branch_1 = x.reshape(x.size(0), x.size(1), 1, 1) * mask\n",
    "    branch_1 = my_MaxPool2d(kernel_size=(1, cnum), stride=(1, cnum))(branch_1)\n",
    "    branch_1 = branch_1.view(branch_1.size(0), -1)\n",
    "    loss_1 = nn.CrossEntropyLoss()(branch_1, targets)\n",
    "    return [loss_1, loss_2]\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, pretrained=True, num_classes=8, drop_rate=0.0, convnext_name=CONVNEXT_NAME):\n",
    "        super(Model, self).__init__()\n",
    "        self.rgb_backbone = timm.create_model(convnext_name, pretrained=pretrained, num_classes=0)\n",
    "        feature_dim = self.rgb_backbone.num_features  # convnext_large는 1536\n",
    "\n",
    "        self.depth_backbone = timm.create_model(convnext_name, pretrained=pretrained, num_classes=0)\n",
    "        depth_conv_weight = self.depth_backbone.stem[0].weight.data.mean(dim=1, keepdim=True)\n",
    "        out_ch = depth_conv_weight.shape[0]\n",
    "        self.depth_backbone.stem[0] = nn.Conv2d(1, out_ch, kernel_size=4, stride=4, padding=0, bias=False)\n",
    "        self.depth_backbone.stem[0].weight.data = depth_conv_weight\n",
    "\n",
    "        self.depth_attention = nn.Sequential(\n",
    "            nn.Linear(512, feature_dim // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(feature_dim // 2, feature_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.image_proj = nn.Linear(512, feature_dim)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.fc = nn.Linear(feature_dim*2, num_classes)\n",
    "\n",
    "    def forward(self, x, clip_model, targets, phase='train'):\n",
    "        rgb_image = x[:, :3, :, :]\n",
    "        depth_map = x[:, 3:, :, :]\n",
    "\n",
    "        rgb_for_clip = F.interpolate(rgb_image, size=(224,224), mode='bicubic', align_corners=False)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_features = clip_model.encode_image(rgb_for_clip)  # (B,512)\n",
    "\n",
    "        image_features = image_features.float()\n",
    "        rgb_feats = self.rgb_backbone(rgb_image)        # (B,1536)\n",
    "        depth_feats = self.depth_backbone(depth_map)    # (B,1536)\n",
    "\n",
    "        depth_att = self.depth_attention(image_features)  # (B,1536)\n",
    "        depth_feats = depth_feats * depth_att\n",
    "\n",
    "        image_features_1024 = self.image_proj(image_features)  # (B,1536)\n",
    "        rgb_feats = rgb_feats * torch.sigmoid(image_features_1024)\n",
    "\n",
    "        combined_features = torch.cat((rgb_feats, depth_feats), dim=1)  # (B, 3072)\n",
    "        combined_features = self.dropout(combined_features)\n",
    "\n",
    "        if phase == 'train':\n",
    "            MC_loss = supervisor(combined_features, targets, cnum=384)  # 3072/8=384\n",
    "        out = self.fc(combined_features)\n",
    "\n",
    "        if phase == 'train':\n",
    "            return out, MC_loss\n",
    "        else:\n",
    "            return out, out\n",
    "\n",
    "def mixup_data(x, y, alpha=1.0):\n",
    "    if alpha <= 0:\n",
    "        return x, y, y, 1.0\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "class SplitApplyTransform:\n",
    "    def __init__(self, rgb_transform=None, depth_transform=None):\n",
    "        self.rgb_transform = rgb_transform\n",
    "        self.depth_transform = depth_transform\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        # img: (H,W,4) -> ToTensor() 후 (C,H,W), C=4\n",
    "        if not isinstance(img, torch.Tensor):\n",
    "            raise TypeError(\"SplitApplyTransform expects a torch.Tensor after ToTensor.\")\n",
    "        rgb = img[:3, :, :]\n",
    "        depth = img[3:, :, :]\n",
    "\n",
    "        if self.rgb_transform is not None:\n",
    "            rgb = self.rgb_transform(rgb)\n",
    "\n",
    "        if self.depth_transform is not None:\n",
    "            depth = self.depth_transform(depth)\n",
    "\n",
    "        return torch.cat((rgb, depth), dim=0)\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.raf_path = '/home/work/dhkim/fer/FERPlus/data'\n",
    "        self.rafdb_test_path = '/home/work/dhkim/fer/Depth-Anything-V2/raf-db/DATASET/test/'\n",
    "        self.label_path = 'list_patition_label.txt'\n",
    "        self.workers = 2\n",
    "        self.batch_size = 64\n",
    "        self.w = 7\n",
    "        self.h = 7\n",
    "        self.gpu = 0\n",
    "        self.lam = 5\n",
    "        self.epochs = 20\n",
    "        self.mixup_alpha = 0.2\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# PIL 기반 RandAugment를 RGB 채널에만 적용하기 위한 transform\n",
    "rgb_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    # 여기서 Resize를 할 필요가 없다면 생략 가능. 필요시 유지.\n",
    "    transforms.Resize((256,256), antialias=True),\n",
    "    transforms.RandAugment(num_ops=2, magnitude=10),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "depth_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((256,256), antialias=True),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),  \n",
    "    SplitApplyTransform(rgb_transform=rgb_transform, depth_transform=depth_transform),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406, 0.5],\n",
    "                         std=[0.229, 0.224, 0.225, 0.5]),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomErasing(scale=(0.02, 0.25))\n",
    "])\n",
    "\n",
    "eval_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    SplitApplyTransform(\n",
    "        rgb_transform=transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((256,256), antialias=True),\n",
    "            transforms.ToTensor()\n",
    "        ]),\n",
    "        depth_transform=transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((256,256), antialias=True),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "    ),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406, 0.5],\n",
    "                         std=[0.229, 0.224, 0.225, 0.5])\n",
    "])\n",
    "\n",
    "def train(args, model, train_loader, optimizer, scheduler, device, scaler, criterion):\n",
    "    running_loss = 0.0\n",
    "    iter_cnt = 0\n",
    "    correct_sum = 0\n",
    "\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for batch_i, (imgs1, labels, indexes, imgs2) in enumerate(train_loader):\n",
    "        imgs1 = imgs1.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        imgs1, y_a, y_b, lam = mixup_data(imgs1, labels, alpha=args.mixup_alpha)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output, MC_loss = model(imgs1, clip_model, y_a, phase='train')\n",
    "            loss1 = mixup_criterion(criterion, output, y_a, y_b, lam)\n",
    "            loss = loss1 + 5 * MC_loss[1] + 1.5 * MC_loss[0]\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        iter_cnt += 1\n",
    "        _, predicts = torch.max(output, 1)\n",
    "        correct_num = torch.eq(predicts, y_a).sum()\n",
    "        correct_sum += correct_num\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    running_loss = running_loss / iter_cnt\n",
    "    acc = correct_sum.float() / float(len(train_loader.dataset))\n",
    "    return acc, running_loss\n",
    "\n",
    "def test(model, test_loader, device, criterion):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        iter_cnt = 0\n",
    "        correct_sum = 0\n",
    "        data_num = 0\n",
    "\n",
    "        for batch_i, (imgs1, labels, indexes, imgs2) in enumerate(test_loader):\n",
    "            imgs1 = imgs1.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs, _ = model(imgs1, clip_model, labels, phase='Test')\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            iter_cnt += 1\n",
    "            _, predicts = torch.max(outputs, 1)\n",
    "            correct_num = torch.eq(predicts, labels).sum()\n",
    "            correct_sum += correct_num\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            data_num += outputs.size(0)\n",
    "\n",
    "        running_loss = running_loss / iter_cnt\n",
    "        test_acc = correct_sum.float() / float(data_num)\n",
    "    return test_acc, running_loss\n",
    "\n",
    "def main():\n",
    "    setup_seed(3407)\n",
    "\n",
    "    train_dataset = RafDataset(root_dir=args.raf_path, phase='Train', transform=train_transforms, apply_constraints=True)\n",
    "    test_dataset = RafDataset(args.raf_path, phase='Test', transform=eval_transforms, apply_constraints=True)\n",
    "    val_dataset = RafDataset(args.raf_path, phase='Valid', transform=eval_transforms, apply_constraints=True)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=args.batch_size,\n",
    "                                               shuffle=True,\n",
    "                                               num_workers=args.workers,\n",
    "                                               pin_memory=False)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=args.batch_size,\n",
    "                                             shuffle=False,\n",
    "                                             num_workers=args.workers,\n",
    "                                             pin_memory=False)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size,\n",
    "                                              shuffle=False,\n",
    "                                              num_workers=args.workers,\n",
    "                                              pin_memory=False)\n",
    "\n",
    "    model = Model(num_classes=8, convnext_name=CONVNEXT_NAME)\n",
    "    device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1)\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    best_acc = 0\n",
    "    for i in range(1, args.epochs + 1):\n",
    "        train_acc, train_loss = train(args, model, train_loader, optimizer, scheduler, device, scaler, criterion)\n",
    "        val_acc, val_loss = test(model, val_loader, device, criterion)\n",
    "        test_acc, test_loss = test(model, test_loader, device, criterion)\n",
    "\n",
    "        print(f\"Epoch: {i}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save({'model_state_dict': model.state_dict()}, \"ours_best.pth\")\n",
    "        torch.save({'model_state_dict': model.state_dict()}, \"ours_final.pth\")\n",
    "\n",
    "    print(\"\\n[Final Evaluation on FERPlus Test Set]\")\n",
    "    test_acc, test_loss = test(model, test_loader, device, criterion)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf0b24e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Re-Evaluation on FERPlus Test Set with Best Model]\n",
      "Test Loss: 0.7732, Test Accuracy: 0.8900\n"
     ]
    }
   ],
   "source": [
    "test_dataset = RafDataset(args.raf_path, phase='Test', transform=eval_transforms, apply_constraints=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size,\n",
    "                                              shuffle=False,\n",
    "                                              num_workers=args.workers,\n",
    "                                              pin_memory=False)\n",
    "\n",
    "\n",
    "\n",
    "rafdb_test_transforms = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((224, 224), antialias=True),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406, 0.5],\n",
    "                             std=[0.229, 0.224, 0.225, 0.5])\n",
    "    ])\n",
    "rafdb_test_dataset = RAFDBTestDataset(root_dir=args.rafdb_test_path, transform=rafdb_test_transforms)\n",
    "rafdb_test_loader = torch.utils.data.DataLoader(rafdb_test_dataset,\n",
    "                                                   batch_size=args.batch_size,\n",
    "                                                   shuffle=False,\n",
    "                                                   num_workers=0,  \n",
    "                                                   pin_memory=False)\n",
    "\n",
    "model = Model(num_classes=8, convnext_name=CONVNEXT_NAME)\n",
    "device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() else 'cpu')\n",
    "checkpoint = torch.load(\"ours_best.pth\", map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_acc, test_loss = test(model, test_loader, device, criterion)\n",
    "    print(f\"[Re-Evaluation on FERPlus Test Set with Best Model]\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917dfcb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.2 (NGC 23.11/Python 3.10) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
