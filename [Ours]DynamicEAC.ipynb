{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "from PIL import Image  # PIL 추가\n",
    "import sys\n",
    "sys.path.append('/kaggle/input/eac2/pytorch/default/1/Erasing-Attention-Consistency/src')  \n",
    "\n",
    "\n",
    "# Utils\n",
    "def add_g(image_array, mean=0.0, var=30):\n",
    "    std = var ** 0.5\n",
    "    image_add = image_array + np.random.normal(mean, std, image_array.shape)\n",
    "    image_add = np.clip(image_add, 0, 255).astype(np.uint8)\n",
    "    return image_add\n",
    "\n",
    "def flip_image(image_array):\n",
    "    return cv2.flip(image_array, 1)\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "def generate_flip_grid(w, h, device):\n",
    "    x_ = torch.arange(w).view(1, -1).expand(h, -1)\n",
    "    y_ = torch.arange(h).view(-1, 1).expand(-1, w)\n",
    "    grid = torch.stack([x_, y_], dim=0).float().to(device)\n",
    "    grid = grid.unsqueeze(0).expand(1, -1, -1, -1)\n",
    "    grid[:, 0, :, :] = 2 * grid[:, 0, :, :] / (w - 1) - 1\n",
    "    grid[:, 1, :, :] = 2 * grid[:, 1, :, :] / (h - 1) - 1\n",
    "    grid[:, 0, :, :] = -grid[:, 0, :, :]\n",
    "    return grid\n",
    "\n",
    "# Dataset\n",
    "class FERPlusDataset(data.Dataset):\n",
    "    def __init__(self, data_dir, label_file=None, phase=\"train\", transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.phase = phase\n",
    "        self.transform = transform\n",
    "\n",
    "        if phase == \"test\":\n",
    "            # 테스트 데이터의 경우 라벨 파일 없이 이미지 파일만 읽음\n",
    "            self.image_paths = sorted([\n",
    "                os.path.join(data_dir, fname)\n",
    "                for fname in os.listdir(data_dir)\n",
    "                if fname.endswith('.png') and os.path.isfile(os.path.join(data_dir, fname))\n",
    "            ])\n",
    "        else:\n",
    "            # 훈련 또는 검증 데이터의 경우 라벨 파일을 사용\n",
    "            self.labels = pd.read_csv(label_file)\n",
    "            self.image_paths = self.labels.iloc[:, 0].apply(lambda x: os.path.join(self.data_dir, x)).values\n",
    "            self.labels = self.labels.iloc[:, 2:].values.argmax(axis=1)  # 라벨은 최대값 인덱스로 추출\n",
    "    \n",
    "    def _apply_constraints(self):\n",
    "        # Constraint : 'unknown-face' 또는 'not-face' 레이블 제거\n",
    "        max_counts = self.counts.max(axis=1)\n",
    "        counts_eq_max = (self.counts == max_counts[:, None])\n",
    "        constraint1_violation = counts_eq_max[:, [8, 9]].any(axis=1)\n",
    "\n",
    "        # Constraint : 1인 라벨 0으로 만들기\n",
    "\n",
    "\n",
    "        # Constraint : 최대 투표 수를 가진 레이블이 3개 초과 제거\n",
    "        num_max_labels = counts_eq_max.sum(axis=1)\n",
    "        constraint2_violation = num_max_labels > 3\n",
    "\n",
    "        # Constraint : 최대 투표 수가 전체 투표 수의 절반 이하인 경우 제거\n",
    "        total_votes = self.counts.sum(axis=1)\n",
    "        constraint3_violation = max_counts <= (total_votes / 2)\n",
    "\n",
    "        # Combine constraints\n",
    "        valid_samples = ~(\n",
    "            constraint1_violation | constraint2_violation | constraint3_violation\n",
    "        )\n",
    "\n",
    "        # Apply valid samples filter\n",
    "        self.file_paths = self.file_paths[valid_samples]\n",
    "        self.counts = self.counts[valid_samples]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        if self.phase == \"train\":\n",
    "            flipped_image = image.transpose(Image.FLIP_LEFT_RIGHT)  # 플립된 이미지 생성\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "                flipped_image = self.transform(flipped_image)\n",
    "\n",
    "            label = self.labels[idx]\n",
    "            label = torch.tensor(label, dtype=torch.long)\n",
    "            return image, label, flipped_image\n",
    "\n",
    "        elif self.phase == \"valid\":\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "\n",
    "            label = self.labels[idx]\n",
    "            label = torch.tensor(label, dtype=torch.long)\n",
    "            return image, label\n",
    "\n",
    "        else:  # self.phase == \"test\"\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "\n",
    "            return image, os.path.basename(image_path)\n",
    "            \n",
    "    @staticmethod\n",
    "    def flip_image(image):\n",
    "        \"\"\"이미지 수평 반전\"\"\"\n",
    "        return cv2.flip(image, 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def add_noise(image, mean=0.0, var=30.0):\n",
    "        \"\"\"이미지에 가우시안 노이즈 추가\"\"\"\n",
    "        std = var ** 0.5\n",
    "        noisy_image = image + np.random.normal(mean, std, image.shape)\n",
    "        noisy_image = np.clip(noisy_image, 0, 255).astype(np.uint8)\n",
    "        return noisy_image\n",
    "\n",
    "    def apply_advanced_aug(self, image):\n",
    "        \"\"\"Advanced augmentation 적용\"\"\"\n",
    "        pil_image = transforms.ToPILImage()(image)  # OpenCV 이미지를 PIL 이미지로 변환\n",
    "        augmented_image = self.advanced_aug(pil_image)  # Advanced augmentation 적용\n",
    "        return np.array(augmented_image)  # 다시 numpy 배열로 변환\n",
    "    def enable_erasing(self):\n",
    "        \"\"\"Dynamic Erasing을 활성화\"\"\"\n",
    "        self.apply_erasing = True\n",
    "\n",
    "    def disable_erasing(self):\n",
    "        \"\"\"Dynamic Erasing을 비활성화\"\"\"\n",
    "        self.apply_erasing = False\n",
    "\n",
    "\n",
    "\n",
    "# ACLoss\n",
    "def ACLoss(att_map1, att_map2, grid_l, output):\n",
    "    flip_grid_large = grid_l.expand(output.size(0), -1, -1, -1)\n",
    "    flip_grid_large = Variable(flip_grid_large, requires_grad=False).permute(0, 2, 3, 1)\n",
    "    att_map2_flip = F.grid_sample(att_map2, flip_grid_large, mode='bilinear', padding_mode='border', align_corners=True)\n",
    "    flip_loss_l = F.mse_loss(att_map1, att_map2_flip)\n",
    "    return flip_loss_l\n",
    "\n",
    "def RegularizationLoss(model, reg_type=\"L2\", lambda_reg=1e-4):\n",
    "    reg_loss = 0.0\n",
    "    for param in model.parameters():\n",
    "        if reg_type == \"L2\":\n",
    "            reg_loss += torch.sum(param ** 2)\n",
    "        elif reg_type == \"L1\":\n",
    "            reg_loss += torch.sum(torch.abs(param))\n",
    "    return lambda_reg * reg_loss\n",
    "\n",
    "def SparsityLoss(feature_map, lambda_s=1e-3):\n",
    "    sparsity_loss = torch.sum(torch.abs(feature_map))\n",
    "    return lambda_s * sparsity_loss\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=8631, include_top=True):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        self.include_top = include_top\n",
    "        self.apply_erasing = False  # Dynamic Erasing 활성화 여부 플래그\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=0, ceil_mode=True)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = [block(self.inplanes, planes, stride, downsample)]\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        # Dynamic Erasing: 상위 10% 활성화 값을 지움\n",
    "        if self.apply_erasing:\n",
    "            x = self.dynamic_erasing_top_k(x, top_k=0.1)\n",
    "\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        if not self.include_top:\n",
    "            return x\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, args, pretrained=True, num_classes=7):\n",
    "        super(Model, self).__init__()\n",
    "        # ResNet50 모델 초기화\n",
    "        resnet50 = ResNet(Bottleneck, [3, 4, 6, 3])\n",
    "        with open(args.resnet50_path, 'rb') as f:\n",
    "            obj = f.read()\n",
    "        weights = {key: torch.from_numpy(arr) for key, arr in pickle.loads(obj, encoding='latin1').items()}\n",
    "        resnet50.load_state_dict(weights)\n",
    "\n",
    "        # 필요한 레이어 구성\n",
    "        self.features = nn.Sequential(*list(resnet50.children())[:-2])  \n",
    "        self.features2 = nn.Sequential(*list(resnet50.children())[-2:-1])  \n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "    def forward(self, x):        \n",
    "        x = self.features(x)\n",
    "        feature = self.features2(x)\n",
    "        feature = feature.view(feature.size(0), -1)\n",
    "        output = self.fc(feature)\n",
    "        params = list(self.parameters())\n",
    "        fc_weights = params[-2].data.view(1, 7, 2048, 1, 1)\n",
    "        fc_weights = Variable(fc_weights, requires_grad=False)\n",
    "        feat = x.unsqueeze(1)\n",
    "        hm = feat * fc_weights\n",
    "        hm = hm.sum(2)\n",
    "        return output, hm\n",
    "    def enable_erasing(self):\n",
    "        \"\"\"Dynamic Erasing을 활성화\"\"\"\n",
    "        self.apply_erasing = True\n",
    "\n",
    "    def disable_erasing(self):\n",
    "        \"\"\"Dynamic Erasing을 비활성화\"\"\"\n",
    "        self.apply_erasing = False\n",
    "\n",
    "# Args Class\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.data_dir = \"/kaggle/input/fer-competition\"\n",
    "        self.train_dir = os.path.join(self.data_dir, \"FER2013Train/FER2013Train\")\n",
    "        self.valid_dir = os.path.join(self.data_dir, \"FER2013Valid/FER2013Valid\")\n",
    "        self.test_dir = os.path.join(self.data_dir, \"FER2013Test/FER2013Test\")  \n",
    "        self.train_labels = os.path.join(self.data_dir, \"train_label.csv\")\n",
    "        self.valid_labels = os.path.join(self.data_dir, \"valid_label.csv\")\n",
    "        self.resnet50_path = \"/kaggle/input/resnet50/resnet50_ft_weight (2).pkl\"\n",
    "        self.batch_size = 32\n",
    "        self.workers = 4\n",
    "        self.epochs = 20\n",
    "        self.gpu = 0\n",
    "        self.w = 7\n",
    "        self.h = 7\n",
    "        self.lam = 5\n",
    "\n",
    "# Training Function\n",
    "def train(args, model, train_loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_sum = 0\n",
    "    total = 0\n",
    "\n",
    "    progress_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "    for imgs1, labels, flipped_imgs in progress_bar:\n",
    "        imgs1, flipped_imgs, labels = imgs1.to(device), flipped_imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output, hm1 = model(imgs1)\n",
    "        output_flip, hm2 = model(flipped_imgs)\n",
    "        grid_l = generate_flip_grid(args.w, args.h, device)\n",
    "\n",
    "        loss1 = nn.CrossEntropyLoss()(output, labels)\n",
    "        flip_loss_l = ACLoss(hm1, hm2, grid_l, output)\n",
    "\n",
    "        loss = loss1 + args.lam * flip_loss_l\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * imgs1.size(0)\n",
    "        _, preds = torch.max(output, 1)\n",
    "        correct_sum += (preds == labels).sum().item()\n",
    "        total += imgs1.size(0)\n",
    "\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    scheduler.step()\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct_sum / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def create_submission_file(args, model, test_loader, device, output_file=\"submission.csv\"):\n",
    "    \"\"\"\n",
    "    테스트 데이터를 기반으로 제출 파일 생성\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (images, _) in enumerate(tqdm(test_loader, desc=\"Generating Submission\")):\n",
    "            images = images.to(device)\n",
    "            outputs, _ = model(images) \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            # 제출용 데이터 생성\n",
    "            for i, pred in enumerate(preds.cpu().numpy()):\n",
    "                predictions.append({\"ID\": len(predictions), \"Prediction\": pred})\n",
    "\n",
    "    # 제출 파일 저장\n",
    "    submission_df = pd.DataFrame(predictions)\n",
    "    submission_df.to_csv(output_file, index=False)\n",
    "    print(f\"Submission file saved to {output_file}\")\n",
    "\n",
    "\n",
    "    \n",
    "# Testing Function\n",
    "def test(args, model, test_loader, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_sum = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(test_loader, desc=\"Testing\", leave=False)\n",
    "        for batch in progress_bar:\n",
    "            if len(batch) == 3:  # train/valid 데이터셋 (image, label, flipped image)\n",
    "                imgs1, labels, _ = batch\n",
    "            elif len(batch) == 2:  # test 데이터셋 (image, filename 또는 label 없음)\n",
    "                imgs1, labels = batch\n",
    "\n",
    "            imgs1, labels = imgs1.to(device), labels.to(device)\n",
    "            output, _ = model(imgs1)\n",
    "            loss = nn.CrossEntropyLoss()(output, labels)\n",
    "\n",
    "            running_loss += loss.item() * imgs1.size(0)\n",
    "            _, preds = torch.max(output, 1)\n",
    "            correct_sum += (preds == labels).sum().item()\n",
    "            total += imgs1.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct_sum / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "# Main Function\n",
    "def main():\n",
    "    setup_seed(0)\n",
    "    args = Args()\n",
    "\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        transforms.RandomErasing(scale=(0.02, 0.25))\n",
    "    ])\n",
    "    eval_transforms = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    train_dataset = FERPlusDataset(args.train_dir, args.train_labels, phase='train', transform=train_transforms)\n",
    "    valid_dataset = FERPlusDataset(args.valid_dir, args.valid_labels, phase='valid', transform=eval_transforms)  \n",
    "    test_dataset = FERPlusDataset(args.test_dir, phase='test', transform=eval_transforms)\n",
    "    \n",
    "    train_loader = data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.workers, pin_memory=True)\n",
    "    valid_loader = data.DataLoader(valid_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=True)\n",
    "    test_loader = data.DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=True)\n",
    "\n",
    "    model = Model(args)\n",
    "    device = torch.device(f\"cuda:{args.gpu}\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "    print(\"Training...\")\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        if epoch == 5: \n",
    "            model.enable_erasing()\n",
    "            print(\"Dynamic Erasing Enabled\")\n",
    "        print(f\"Epoch {epoch}/{args.epochs}\")\n",
    "        train_loss, train_acc = train(args, model, train_loader, optimizer, scheduler, device)\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    \n",
    "\n",
    "    train_acc, train_loss = train(args, model, train_loader, optimizer, scheduler, device)\n",
    "    val_loss, val_acc = test(args, model, valid_loader, device)\n",
    "    print(f\"Epoch {epoch}: Train Acc: {train_acc:.4f}, Train Loss: {train_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "\n",
    "    print(\"Generating Submission...\")\n",
    "    create_submission_file(args, model, test_loader, device)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
